FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999994039535520000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000005960464480000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 4, 5.00000000000000000000e-001) (9, 4, 5.00000000000000000000e-001) (9, 4, 5.00000000000000000000e-001) (9, 4, 5.00000000000000000000e-001) (9, 4, 5.00000000000000000000e-001) (9, 4, 5.00000000000000000000e-001) (9, 4, 5.00000000000000000000e-001) (0, 0, 0.00000000000000000000e+000) (8, 4, 5.00000000000000000000e-001) (8, 4, 5.00000000000000000000e-001) (8, 4, 5.00000000000000000000e-001) (0, 0, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.45837394893169400000e-002) (1, -9.42443832755088810000e-002) (2, -1.70715339481830600000e-002) (3, 8.42590332031250000000e-002) (4, 7.00683612376451490000e-003) (5, -1.73339843750000000000e-002) (6, 5.16296401619911190000e-002) (7, 7.40295425057411190000e-002) (8, -3.22998054325580600000e-002) (0, -4.21386733651161190000e-002) (1, 1.24023435637354850000e-002) (2, -4.20227050781250000000e-002) (3, -5.91491721570491790000e-002) (4, -8.05664062500000000000e-003) (5, -3.59558127820491790000e-002) (6, -1.94702146109193560000e-003) (7, -3.42834480106830600000e-002) (8, -8.19091778248548510000e-003) (0, 1.17187500000000000000e-002) (1, -9.88952666521072390000e-002) (2, -4.53247092664241790000e-002) (3, 2.16369628906250000000e-002) (4, 5.91003410518169400000e-002) (5, -3.24646010994911190000e-002) (6, 7.22168013453483580000e-002) (7, 9.07775908708572390000e-002) (8, 6.96166977286338810000e-002) (0, 3.25622558593750000000e-002) (1, -6.37573227286338810000e-002) (2, 7.48962387442588810000e-002) (3, -7.19055160880088810000e-002) (4, 6.25000009313225750000e-003) (5, 2.29003913700580600000e-002) (6, 2.65380870550870900000e-002) (7, -6.59545883536338810000e-002) (8, -8.13293457031250000000e-002) (0, -4.38720695674419400000e-002) (1, 3.13842780888080600000e-002) (2, 5.78491203486919400000e-002) (3, 7.63671919703483580000e-002) (4, 6.32751509547233580000e-002) (5, 6.64978027343750000000e-002) (6, -8.93554743379354480000e-003) (7, 3.64074707031250000000e-002) (8, 7.79174789786338810000e-002) (0, 1.67602542787790300000e-002) (1, -7.78625532984733580000e-002) (2, 4.26025409251451490000e-003) (3, -2.87231449037790300000e-002) (4, -9.42626968026161190000e-002) (5, 6.87438994646072390000e-002) (6, -3.82812507450580600000e-002) (7, -6.67114276438951490000e-003) (8, 9.72656235098838810000e-002) (0, 9.80590805411338810000e-002) (1, -6.87561035156250000000e-002) (2, -9.50317382812500000000e-002) (3, -7.59887695312500000000e-002) (4, 6.60888701677322390000e-002) (5, 8.27636718750000000000e-002) (6, 8.24035629630088810000e-002) (7, 9.91943404078483580000e-002) (8, 7.47680664062500000000e-003) (9, 8.94042998552322390000e-002) (10, 7.51403793692588810000e-002) (11, -3.22021506726741790000e-002) (12, 6.90917996689677240000e-003) (13, 3.82263176143169400000e-002) (14, 9.86328125000000000000e-002) (15, -4.65332046151161190000e-002) (16, 8.34106430411338810000e-002) (9, -9.76623520255088810000e-002) (10, 4.30297851562500000000e-002) (11, 5.73486350476741790000e-002) (12, -8.12194868922233580000e-002) (13, 8.05725082755088810000e-002) (14, 7.91320800781250000000e-002) (15, -3.48388664424419400000e-002) (16, -3.85559089481830600000e-002) (9, 1.09619144350290300000e-002) (10, 4.29199226200580600000e-002) (11, 1.86889655888080600000e-002) (12, -6.30981475114822390000e-002) (13, -3.56506370007991790000e-002) (14, 7.97851607203483580000e-002) (15, 8.00781231373548510000e-003) (16, 9.61181670427322390000e-002) 
